{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival with Decision Tree\n",
    "\n",
    "***How probable would you have survived in such catastrophy?*** What impacts in your chance of survival?\n",
    "\n",
    "This notebook aims to show a quick application of decision forest, therefore uses sklearn library, which has decision tree built in. If anyone is interested on how to build one from scratch, there is a notebook in this folder which has the complete code.\n",
    "\n",
    "This problem and data are taken from Kaggle [*Titanic - Machine Learning from Disaster*](https://www.kaggle.com/c/titanic/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the train and test data, they are already in separate files\n",
    "# SibSp are the number of siblings abroad, Parch the number of parent/children abroad, Embarked is the first letter of embarked city\n",
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting information about the data\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at statistical summary\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess our data and build our label encoders and data after split\n",
    "label_encoders = {}\n",
    "\n",
    "def preprocess(df, encoded=False):\n",
    "    \n",
    "    # Drop Cabin column because too many missing values\n",
    "    # Drop Name and Ticket column as they are not a suitable in this case\n",
    "    # Drop rows with missing Age and Embarked city\n",
    "    df = df.drop(columns=[\"Name\"]).drop(columns=[\"Ticket\"]).drop(columns=[\"Cabin\"]).dropna()\n",
    "\n",
    "\n",
    "    # Preprocessing string values into labels\n",
    "    for attribute in [\"Sex\", \"Embarked\"]:\n",
    "\n",
    "        # Train data builds the label encoder\n",
    "        if (not(encoded)):\n",
    "            le = LabelEncoder()\n",
    "            df[attribute] = le.fit_transform(df[attribute])\n",
    "            label_encoders[attribute] = le\n",
    "\n",
    "        # This used to process test data when label encoders are already built\n",
    "        else:\n",
    "            df[attribute] = label_encoders[attribute].transform(df[attribute])\n",
    "\n",
    "\n",
    "    # Split the data into x and y where x are the features/attributes and y whether survived or not\n",
    "    if (not(encoded)):\n",
    "        return df.iloc[:, 2:], df.iloc[:, 1:2]\n",
    "    # Test data only has x\n",
    "    else:\n",
    "        return df.iloc[:, 1:]\n",
    "        \n",
    "\n",
    "\n",
    "if \"Cabin\" in train_data.columns:\n",
    "    train_data_x, train_data_y = preprocess(train_data)\n",
    "train_data_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_x.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a decision tree by GINI index\n",
    "decision_tree = DecisionTreeClassifier(criterion=\"gini\", random_state=49)\n",
    "\n",
    "\n",
    "# Train it with train data, omit headers\n",
    "decision_tree.fit(train_data_x.values,train_data_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick prediction given made up conditions\n",
    "customised = {\n",
    "    \"Pclass\": 3, # Integer: 1/2/3\n",
    "    \"Sex\": \"female\", # String: male/female\n",
    "    \"Age\": 20, # Float\n",
    "    \"SibSp\": 5, # Integer\n",
    "    \"Parch\": 2, # Integer\n",
    "    \"Fare\": 250, # Float\n",
    "    \"Embarked\": \"C\" # String: Q/C/S\n",
    "}\n",
    "\n",
    "predict = []\n",
    "\n",
    "# Label string sttributes\n",
    "for key in customised:\n",
    "    if key in label_encoders.keys():\n",
    "        predict.append(label_encoders[key].transform([customised[key]]))\n",
    "    else:\n",
    "        predict.append(customised[key])\n",
    "\n",
    "# Make a prediction\n",
    "prediction = decision_tree.predict([predict])[0]\n",
    "\n",
    "s = \"\"\"Would someone \n",
    " - with a {} class ticket\n",
    " - sex is {}\n",
    " - is {}\n",
    " - has {} siblings and {} parents/childs abroad\n",
    " - with a ticket fare of {}\n",
    " - embarked from {}\n",
    " HAVE SURVIVED?\n",
    " Decision Tree's answer is {}.\"\"\".format(\n",
    " (\"first\" if customised[\"Pclass\"] == 1 else (\"second\" if customised[\"Pclass\"] == 2 else \"third\")),\n",
    " customised[\"Sex\"],\n",
    " (str(int(customised[\"Age\"])) + \" years old\") if int(customised[\"Age\"])>1 else ((\"1 year old\") if int(customised[\"Age\"]) == 1 else \"a baby less than a year old\"),\n",
    " str(customised[\"SibSp\"]),\n",
    " str(customised[\"Parch\"]),\n",
    " str(round(customised[\"Fare\"], 2)),\n",
    " \"Cherbourg\" if customised[\"Embarked\"] == \"C\" else (\"Queenstown\" if (customised[\"Embarked\"] == \"Q\") else \"Southampton\"),\n",
    " \"YES\" if prediction == 1 else \"no\")\n",
    "\n",
    "print(s)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same preprocessing with test data\n",
    "test_data = pd.read_csv(\"./data/test.csv\")\n",
    "test_data_x = preprocess(test_data, encoded=True)\n",
    "\n",
    "# Read in solution data and drop unmatching rows\n",
    "test_data_y = pd.read_csv(\"./data/solution.csv\")\n",
    "unmatching_rows = [i for i in test_data_y.index if i not in test_data_x.index]\n",
    "test_data_y = test_data_y.drop(unmatching_rows).drop(columns=[\"PassengerId\"])\n",
    "\n",
    "\n",
    "score = decision_tree.score(test_data_x.values, test_data_y)\n",
    "print(\"The decision tree predicts the result correctly in {}%\".format(round(score, 2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "mat = confusion_matrix(decision_tree.predict(test_data_x.values), test_data_y.values)\n",
    "\n",
    "plt.figure(figsize = (16,10))\n",
    "sns.heatmap(mat, annot=True, annot_kws={'size': 15}, square = True, fmt=\".3g\")\n",
    "plt.xticks(size = 15)\n",
    "plt.yticks(size = 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most discriminatory features\n",
    "pd.concat((pd.DataFrame(train_data_x.columns, columns = ['variable']), \n",
    "           pd.DataFrame(decision_tree.feature_importances_, columns = ['importance'])), \n",
    "          axis = 1).sort_values(by='importance', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Survived\"].groupby(train_data[\"Sex\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems like the phrase ***\"Women and children first\"*** is not just a saying, our most sincere respects to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 20))\n",
    "_ = plot_tree(decision_tree, feature_names=train_data_x.columns, class_names=[\"No\", \"Yes\"], filled=True)\n",
    "plt.savefig(\"decision_tree.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tree predicts the survival correctly in *68%*,\n",
    "\n",
    "which is not bad. However, this could be **better**, note that:\n",
    "- The dataset is relatively *small*, fewer than 1000. In addition, we removed part of it, making it even smaller.\n",
    "  - Maybe we can replace the missing values\n",
    "- We didn't do any hyperparameter tunning (e.g. max_depth, min_samples_split)\n",
    "  - Try to cross validate\n",
    "- Decision trees are often overfitting training data, is there any solution?\n",
    "  - Ensemble methods, e.g. Random Forest"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "997a4ac3d91279ec1e690820e037f2049f43afc77ab82c37250ea76238432bd0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival with Random Forest\n",
    "\n",
    "***How probable would you have survived in such catastrophy?*** What impacts in your chance of survival?\n",
    "\n",
    "This notebook aims to show a quick application of random forest, therefore uses sklearn library, which has random forest built in.\n",
    "\n",
    "This problem and data are taken from Kaggle [*Titanic - Machine Learning from Disaster*](https://www.kaggle.com/c/titanic/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function\n",
    "label_encoders = {}\n",
    "\n",
    "def preprocess(df, encoded=False):\n",
    "    df = df.drop(columns=[\"Name\"]).drop(columns=[\"Ticket\"]).drop(columns=[\"Cabin\"]).dropna()\n",
    "    for attribute in [\"Sex\", \"Embarked\"]:\n",
    "\n",
    "        # Train data builds the label encoder\n",
    "        if (not(encoded)):\n",
    "            le = LabelEncoder()\n",
    "            df[attribute] = le.fit_transform(df[attribute])\n",
    "            label_encoders[attribute] = le\n",
    "\n",
    "        # This used to process test data when label encoders are already built\n",
    "        else:\n",
    "            df[attribute] = label_encoders[attribute].transform(df[attribute])\n",
    "\n",
    "\n",
    "    # Split the data into x and y where x are the features/attributes and y whether survived or not\n",
    "    if (not(encoded)):\n",
    "        return df.iloc[:, 2:], df.iloc[:, 1:2]\n",
    "    # Test data only has x\n",
    "    else:\n",
    "        return df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "if \"Cabin\" in train_data.columns:\n",
    "    train_data_x, train_data_y = preprocess(train_data)\n",
    "    test_data_x = preprocess(test_data, encoded=True)\n",
    "\n",
    "test_data_y = pd.read_csv(\"./data/solution.csv\")\n",
    "unmatching_rows = [i for i in test_data_y.index if i not in test_data_x.index]\n",
    "test_data_y = test_data_y.drop(unmatching_rows).drop(columns=[\"PassengerId\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can customise the number of trees in the random forest\n",
    "number_of_trees = 10\n",
    "random_forest = RandomForestClassifier(n_estimators=number_of_trees, criterion=\"gini\", random_state = 15)\n",
    "random_forest.fit(train_data_x.values, train_data_y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a test, feel free to change variables here\n",
    "customised = {\n",
    "    \"Pclass\": 1, # Integer: 1/2/3\n",
    "    \"Sex\": \"female\", # String: male/female\n",
    "    \"Age\": 19, # Float\n",
    "    \"SibSp\": 1, # Integer\n",
    "    \"Parch\": 2, # Integer\n",
    "    \"Fare\": 520, # Float\n",
    "    \"Embarked\": \"Q\" # String: Q/C/S\n",
    "}\n",
    "\n",
    "predict = []\n",
    "\n",
    "for key in customised:\n",
    "    if key in label_encoders.keys():\n",
    "        predict.append(label_encoders[key].transform([customised[key]]))\n",
    "    else:\n",
    "        predict.append(customised[key])\n",
    "\n",
    "prediction = random_forest.predict([predict])[0]\n",
    "\n",
    "s = \"\"\"Would someone \n",
    " - with a {} class ticket\n",
    " - sex is {}\n",
    " - is {}\n",
    " - has {} siblings and {} parents/childs abroad\n",
    " - with a ticket fare of {}\n",
    " - embarked from {}\n",
    " HAVE SURVIVED?\n",
    " Random Forest's answer is {}.\"\"\".format(\n",
    " (\"first\" if customised[\"Pclass\"] == 1 else (\"second\" if customised[\"Pclass\"] == 2 else \"third\")),\n",
    " customised[\"Sex\"],\n",
    " (str(int(customised[\"Age\"])) + \" years old\") if int(customised[\"Age\"])>1 else ((\"1 year old\") if int(customised[\"Age\"]) == 1 else \"a baby less than a year old\"),\n",
    " str(customised[\"SibSp\"]),\n",
    " str(customised[\"Parch\"]),\n",
    " str(round(customised[\"Fare\"], 2)),\n",
    " \"Cherbourg\" if customised[\"Embarked\"] == \"C\" else (\"Queenstown\" if (customised[\"Embarked\"] == \"Q\") else \"Southampton\"),\n",
    " \"YES\" if prediction == 1 else \"no\")\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we haven't done any hyperparameter tuning, so that we can have a direct comparison in perandom_forestormance\n",
    "score = random_forest.score(test_data_x.values, test_data_y.values)\n",
    "print(\"The random forest predicts the result correctly in {}%\".format(round(score, 2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "mat = confusion_matrix(random_forest.predict(test_data_x.values), test_data_y.values)\n",
    "\n",
    "plt.figure(figsize = (16,10))\n",
    "sns.heatmap(mat, annot=True, annot_kws={'size': 15}, square = True, fmt=\".3g\")\n",
    "plt.xticks(size = 15)\n",
    "plt.yticks(size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot any tree from index 0 to 9(inclusive) in the decision forest\n",
    "i = 0\n",
    "plt.figure(figsize=(25, 20))\n",
    "_ = plot_tree(random_forest.estimators_[i], feature_names=train_data_x.columns, class_names=[\"No\", \"Yes\"], filled=True)\n",
    "plt.savefig(\"random_forest_tree_\"+str(i)+\".svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decision forest predicts the survival correctly in *75%*,\n",
    "\n",
    "Comparing to the previous model of decision tree, decision tree resulted more accurate in *7%*, note that:\n",
    "- The dataset is relatively *small*, fewer than 1000. In addition, we removed part of it, making it even smaller.\n",
    "  - Maybe we can replace the missing values\n",
    "- We didn't do any hyperparameter tunning (e.g. n_estimator, max_depth, min_samples_split)\n",
    "  - Try to cross validate\n",
    "\n",
    "We conclude that random forest is able to optimize a decision tree"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "997a4ac3d91279ec1e690820e037f2049f43afc77ab82c37250ea76238432bd0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
